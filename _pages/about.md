---
permalink: /
title: "Biography"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I'm a researcher in deep learning, now working as a research assistant at Westlake University with [Prof. Stan Z. Li](https://scholar.google.com/citations?user=Y-nyLGIAAAAJ), researching on AI related molecule and protein design.
Prior to this, I received my bachelor's degree in Computer Science & Mathematics from the University of Electronic Science and Technology of China (UESTC).
I also spent a year at Shanghai Artificial Intelligence Laboratory with [Prof. Yu Cheng](https://scholar.google.com/citations?user=ORPxbV4AAAAJ), researching on the Mixture of Experts (MoE) in large language models.

My research topic covers across domains like ML, NLP and CV, and now I am dedicated to uncovering the intrinsic properties of neural networks with theoretical guarantees.
My primary research interests include but not limited to:
1. **Representation Learning** on enhancing the neural networks through representational perspectives (e.g. degradation, collapse).
2. **Neural Network Architecture** that discovers efficient and mathematically complete structures (e.g. MoE, GNN).
3. **AI for Biology / Psychology** to promote the scientific progress of human beings.

<!-- My focus is on the quality and broader impacts of my work, rather than the mere quantity of top-conference papers. -->

<!-- <span style="color: red"> **I am actively looking for a PhD program. If you find our research interests align or if there is potential for collaboration, please feel free to get in touch with me.** </span> -->


## News

- [2024/05] One paper ([GraphsGPT](https://arxiv.org/abs/2402.02464)) is accepted by [ICML 2024](https://icml.cc/Conferences/2024).
- [2024/04] One paper ([iDAT](https://arxiv.org/abs/2403.15750)) is accepted by [ICME 2024](https://icml.cc/Conferences/2024) as oral.
- [2023/12] Open-sourced project [LLaMA-MoE](https://github.com/pjlab-sys4nlp/llama-moe) is released.
- [2023/05] One paper ([PAD-Net](https://aclanthology.org/2023.acl-long.803.pdf)) is accepted by [ACL 2023](https://2023.aclweb.org/).
- [2022/10] One paper ([SparseAdapter](https://aclanthology.org/2022.findings-emnlp.160.pdf)) is accepted by [EMNLP 2022](https://2022.emnlp.org/).
- [2022/08] One paper ([SD-Conv](https://openaccess.thecvf.com/content/WACV2023/papers/He_SD-Conv_Towards_the_Parameter-Efficiency_of_Dynamic_Convolution_WACV_2023_paper.pdf)) is accepted by [WACV 2023](https://wacv2023.thecvf.com/).



## Selected Publications

1. Zhangyang Gao\*, **Daize Dong**\*, Cheng Tan, Jun Xia, Bozhen Hu, Stan Z. Li, ***A Graph is Worth K Words: Euclideanizing Graph using Pure Transformer***, The 41st International Conference on Machine Learning (ICML 2024). [[Paper](https://arxiv.org/abs/2402.02464)]
2. Jiacheng Ruan, Jingsheng Gao, Mingye Xie, **Daize Dong**, Suncheng Xiang, Ting Liu, Yuzhuo Fu, iDAT: inverse Distillation Adapter-Tuning, The 15th International Congress on Mathematical Education (ICME 2024). [[Paper](https://arxiv.org/abs/2403.15750)] **(Oral)**
3. Shwai He, Liang Ding, **Daize Dong**, Boan Liu, Fuqiang Yu, Dacheng Tao, ***PAD-Net: An Efficient Framework for Dynamic Networks***, Proceedings of The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023). [[Paper](https://aclanthology.org/2023.acl-long.803.pdf)]
3. Shwai He, Liang Ding, **Daize Dong**, Miao Zhang, Dacheng Tao, ***SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters***, Findings of The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022). [[Paper](https://aclanthology.org/2022.findings-emnlp.160.pdf)]
4. Shwai He, Chenbo Jiang, **Daize Dong**, Liang Ding, ***SD-Conv: Towards the Parameter-Efficiency of Dynamic Convolution***, IEEE/CVF Winter Conference on Applications of Computer Vision, 2023 (WACV 2023). [[Paper](https://openaccess.thecvf.com/content/WACV2023/papers/He_SD-Conv_Towards_the_Parameter-Efficiency_of_Dynamic_Convolution_WACV_2023_paper.pdf)]



## Selected Projects

1. ***LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training***. [[Paper](https://arxiv.org/abs/2406.16554)] [[Code](https://github.com/pjlab-sys4nlp/llama-moe)]






