---
permalink: /
title: "Biography"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a research assistant at Shanghai Artificial Intelligence Laboratory, supervised by [Prof. Yu Cheng](https://scholar.google.com/citations?user=ORPxbV4AAAAJ). I also hold a research assistant position at Westlake University, working closely with [Prof. Stan Z. Li](https://scholar.google.com/citations?user=Y-nyLGIAAAAJ). Prior to this, I received my bachelor's degree in Computer Science & Mathematics and Applied Mathematics from University of Electronic Science and Technology of China, where I worked as a research intern under the supervision of [Prof. Wen Li](https://wenli-vision.github.io/).

I am dedicated to uncovering the intrinsic properties of neural networks with theoretical guarantees, aiming to develop robust and effective systems. **My research interests primarily revolve around:**
1. Representation capabilities of neural networks (e.g. interpretability, robustness, sparsity).
2. Fundamental structural designs of neural networks (e.g. Mixture of Experts, Graph Neural Networks).
3. Applications of artificial intelligence for scientific research (e.g. AI for physics, biology, psychology).

<!-- My focus is on the quality and broader impacts of my work, rather than the mere quantity of top-conference papers. -->

<span style="color: red"> **I am actively looking for a PhD program. If you find our research interests align or if there is potential for collaboration, please feel free to get in touch with me.** </span>


## News

- [2024/05] One paper is accepted by [ICML 2024](https://icml.cc/Conferences/2024).
- [2023/12] We release [LLaMA-MoE](https://github.com/pjlab-sys4nlp/llama-moe), a series of open-sourced Mixture-of-Expert (MoE) models.
- [2023/05] One paper is accepted by [ACL 2023](https://2023.aclweb.org/).
- [2022/10] One paper is accepted by [EMNLP 2022](https://2022.emnlp.org/).



## Research Experiences

<dl>
  <dt><img align="left" width="100" height="100" hspace="16" src="../images/logo/shanghai_ai_lab2.png" /></dt>
  <dt>Shanghai Artificial Intelligence Laboratory</dt>
  <dd>2023/07 - Now</dd>
  <d>Research Assistant, supervised by <a href="https://scholar.google.com/citations?user=ORPxbV4AAAAJ">Prof. Yu Cheng</a></d>
  <dd>Large Language Models, Natural Language Processing</dd>
</dl>



<dl>
  <dt><img align="left" width="100" height="100" hspace="16" src="../images/logo/westlake.png" /></dt>
  <dt>Westlake University</dt>
  <dd>2023/04 - Now</dd>
  <d>Research Assistant, supervised by <a href="https://scholar.google.com/citations?user=Y-nyLGIAAAAJ">Prof. Stan Z. Li</a></d>
  <dd>Molecular Generation, AI for Drug Discovery and Development</dd>
</dl>



<dl>
  <dt><img align="left" width="100" height="100" hspace="16" src="../images/logo/uestc.png" /></dt>
  <dt>University of Electronic Science and Technology of China</dt>
  <dd>2022/07 - 2023/03</dd>
  <d>Research Intern, supervised by <a href="https://wenli-vision.github.io/">Prof. Wen Li</a></d>
  <dd>Domain Adaptation, Transfer Learning</dd>
</dl>



## Selected Publications

1. Zhangyang Gao\*, **Daize Dong**\*, Cheng Tan, Jun Xia, Bozhen Hu, Stan Z. Li, ***A Graph is Worth K Words: Euclideanizing Graph using Pure Transformer***, The 41st International Conference on Machine Learning (ICML 2024). [[Paper](https://arxiv.org/abs/2402.02464)]
2. Shwai He, Liang Ding, **Daize Dong**, Boan Liu, Fuqiang Yu, Dacheng Tao, ***PAD-Net: An Efficient Framework for Dynamic Networks***, Proceedings of The 61st Annual Meeting of the Association for Computational Linguistics (ACL 2023). [[Paper](https://aclanthology.org/2023.acl-long.803.pdf)]
3. Shwai He, Liang Ding, **Daize Dong**, Miao Zhang, Dacheng Tao, ***SparseAdapter: An Easy Approach for Improving the Parameter-Efficiency of Adapters***, Findings of The 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP 2022). [[Paper](https://aclanthology.org/2022.findings-emnlp.160.pdf)]



## Selected Projects

1. ***LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training***. [[Code](https://github.com/pjlab-sys4nlp/llama-moe)] [[Technical Report](https://github.com/pjlab-sys4nlp/llama-moe/blob/main/docs/LLaMA_MoE.pdf)]






